{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-widedeep in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (1.3.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (4.3.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (3.6.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (4.8.0.76)\n",
      "Requirement already satisfied: imutils in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (0.5.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (4.66.1)\n",
      "Requirement already satisfied: torch in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (0.15.2)\n",
      "Requirement already satisfied: einops in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (0.6.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (1.15.0)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (1.2.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (13.0.0)\n",
      "Requirement already satisfied: fastparquet>=0.8.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pytorch-widedeep) (2023.8.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2.7.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2023.9.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from gensim->pytorch-widedeep) (6.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (0.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from spacy->pytorch-widedeep) (3.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from torch->pytorch-widedeep) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from torch->pytorch-widedeep) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from torch->pytorch-widedeep) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from torch->pytorch-widedeep) (3.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from torchmetrics->pytorch-widedeep) (0.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from torchvision->pytorch-widedeep) (9.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy->pytorch-widedeep) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from jinja2->spacy->pytorch-widedeep) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wei kang\\desktop\\individual assignment\\.venv\\lib\\site-packages (from sympy->torch->pytorch-widedeep) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "\n",
    "def set_seed(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in train_df: 2017, 2018, 2019, 2020\n",
      "Unique years in test_df: 2021, 2022, 2023\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# TODO: Enter your code here\n",
    "\n",
    "# Divide dataset into training and test sets\n",
    "train_df = df[df['year'] <= 2020]\n",
    "test_df = df[df['year'] >= 2021]\n",
    "\n",
    "# Sanity Check: Get unique values \n",
    "train_unique_years = ', '.join(map(str, train_df['year'].unique()))\n",
    "test_unique_years = ', '.join(map(str, test_df['year'].unique()))\n",
    "\n",
    "# Sanity Check: Print the formatted unique values\n",
    "print(f\"Unique years in train_df: {train_unique_years}\")\n",
    "print(f\"Unique years in test_df: {test_unique_years}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 956/956 [00:10<00:00, 94.20it/s, loss=2.49e+5] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 171.47it/s, loss=8.42e+4]\n",
      "epoch 2: 100%|██████████| 956/956 [00:09<00:00, 100.94it/s, loss=8.68e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 175.60it/s, loss=6.24e+4]\n",
      "epoch 3: 100%|██████████| 956/956 [00:10<00:00, 94.77it/s, loss=7.82e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 173.26it/s, loss=5.75e+4]\n",
      "epoch 4: 100%|██████████| 956/956 [00:10<00:00, 94.56it/s, loss=7.47e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.81it/s, loss=5.55e+4]\n",
      "epoch 5: 100%|██████████| 956/956 [00:12<00:00, 77.18it/s, loss=7.3e+4]  \n",
      "valid: 100%|██████████| 410/410 [00:03<00:00, 134.80it/s, loss=5.44e+4]\n",
      "epoch 6: 100%|██████████| 956/956 [00:14<00:00, 66.09it/s, loss=7.11e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 141.31it/s, loss=5.38e+4]\n",
      "epoch 7: 100%|██████████| 956/956 [00:11<00:00, 82.86it/s, loss=6.98e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 139.70it/s, loss=5.32e+4]\n",
      "epoch 8: 100%|██████████| 956/956 [00:11<00:00, 82.71it/s, loss=6.92e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.85it/s, loss=5.29e+4]\n",
      "epoch 9: 100%|██████████| 956/956 [00:11<00:00, 82.92it/s, loss=6.79e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.30it/s, loss=5.26e+4]\n",
      "epoch 10: 100%|██████████| 956/956 [00:11<00:00, 80.80it/s, loss=6.78e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.67it/s, loss=5.25e+4]\n",
      "epoch 11: 100%|██████████| 956/956 [00:11<00:00, 86.43it/s, loss=6.72e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.81it/s, loss=5.25e+4]\n",
      "epoch 12: 100%|██████████| 956/956 [00:11<00:00, 86.36it/s, loss=6.69e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.96it/s, loss=5.22e+4]\n",
      "epoch 13: 100%|██████████| 956/956 [00:11<00:00, 85.67it/s, loss=6.64e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 154.86it/s, loss=5.22e+4]\n",
      "epoch 14: 100%|██████████| 956/956 [00:11<00:00, 83.49it/s, loss=6.65e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.34it/s, loss=5.24e+4]\n",
      "epoch 15: 100%|██████████| 956/956 [00:11<00:00, 84.44it/s, loss=6.59e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 152.84it/s, loss=5.21e+4]\n",
      "epoch 16: 100%|██████████| 956/956 [00:11<00:00, 85.18it/s, loss=6.55e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.66it/s, loss=5.22e+4]\n",
      "epoch 17: 100%|██████████| 956/956 [00:11<00:00, 84.92it/s, loss=6.53e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 160.26it/s, loss=5.21e+4]\n",
      "epoch 18: 100%|██████████| 956/956 [00:11<00:00, 86.82it/s, loss=6.54e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 157.20it/s, loss=5.21e+4]\n",
      "epoch 19: 100%|██████████| 956/956 [00:11<00:00, 82.86it/s, loss=6.5e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 157.40it/s, loss=5.19e+4]\n",
      "epoch 20: 100%|██████████| 956/956 [00:10<00:00, 87.95it/s, loss=6.5e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 153.92it/s, loss=5.2e+4] \n",
      "epoch 21: 100%|██████████| 956/956 [00:11<00:00, 86.70it/s, loss=6.47e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 159.27it/s, loss=5.21e+4]\n",
      "epoch 22: 100%|██████████| 956/956 [00:10<00:00, 87.24it/s, loss=6.46e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 159.59it/s, loss=5.22e+4]\n",
      "epoch 23: 100%|██████████| 956/956 [00:12<00:00, 75.03it/s, loss=6.44e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 154.30it/s, loss=5.21e+4]\n",
      "epoch 24: 100%|██████████| 956/956 [00:11<00:00, 83.47it/s, loss=6.43e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.28it/s, loss=5.19e+4]\n",
      "epoch 25: 100%|██████████| 956/956 [00:11<00:00, 82.85it/s, loss=6.43e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 138.53it/s, loss=5.22e+4]\n",
      "epoch 26: 100%|██████████| 956/956 [00:11<00:00, 82.98it/s, loss=6.41e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 145.62it/s, loss=5.2e+4] \n",
      "epoch 27: 100%|██████████| 956/956 [00:12<00:00, 76.49it/s, loss=6.42e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.00it/s, loss=5.21e+4]\n",
      "epoch 28: 100%|██████████| 956/956 [00:11<00:00, 82.72it/s, loss=6.39e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.59it/s, loss=5.2e+4] \n",
      "epoch 29: 100%|██████████| 956/956 [00:11<00:00, 81.65it/s, loss=6.37e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 140.09it/s, loss=5.19e+4]\n",
      "epoch 30: 100%|██████████| 956/956 [00:11<00:00, 82.94it/s, loss=6.37e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.90it/s, loss=5.19e+4]\n",
      "epoch 31: 100%|██████████| 956/956 [00:11<00:00, 79.96it/s, loss=6.35e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.49it/s, loss=5.18e+4]\n",
      "epoch 32: 100%|██████████| 956/956 [00:11<00:00, 82.30it/s, loss=6.36e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.16it/s, loss=5.2e+4] \n",
      "epoch 33: 100%|██████████| 956/956 [00:11<00:00, 81.51it/s, loss=6.35e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 144.54it/s, loss=5.19e+4]\n",
      "epoch 34: 100%|██████████| 956/956 [00:11<00:00, 82.46it/s, loss=6.32e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.14it/s, loss=5.2e+4] \n",
      "epoch 35: 100%|██████████| 956/956 [00:11<00:00, 81.75it/s, loss=6.3e+4] \n",
      "valid: 100%|██████████| 410/410 [00:03<00:00, 132.60it/s, loss=5.21e+4]\n",
      "epoch 36: 100%|██████████| 956/956 [00:11<00:00, 83.30it/s, loss=6.31e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 145.53it/s, loss=5.21e+4]\n",
      "epoch 37: 100%|██████████| 956/956 [00:11<00:00, 81.82it/s, loss=6.29e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.36it/s, loss=5.19e+4]\n",
      "epoch 38: 100%|██████████| 956/956 [00:11<00:00, 82.98it/s, loss=6.27e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 144.89it/s, loss=5.19e+4]\n",
      "epoch 39: 100%|██████████| 956/956 [00:12<00:00, 79.28it/s, loss=6.26e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 144.32it/s, loss=5.2e+4] \n",
      "epoch 40: 100%|██████████| 956/956 [00:11<00:00, 80.95it/s, loss=6.25e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 154.95it/s, loss=5.19e+4]\n",
      "epoch 41: 100%|██████████| 956/956 [00:11<00:00, 86.33it/s, loss=6.24e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 154.90it/s, loss=5.2e+4] \n",
      "epoch 42: 100%|██████████| 956/956 [00:11<00:00, 79.86it/s, loss=6.22e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.44it/s, loss=5.18e+4]\n",
      "epoch 43: 100%|██████████| 956/956 [00:11<00:00, 82.49it/s, loss=6.22e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.25it/s, loss=5.21e+4]\n",
      "epoch 44: 100%|██████████| 956/956 [00:11<00:00, 82.33it/s, loss=6.22e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.03it/s, loss=5.18e+4]\n",
      "epoch 45: 100%|██████████| 956/956 [00:11<00:00, 82.70it/s, loss=6.2e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.44it/s, loss=5.2e+4] \n",
      "epoch 46: 100%|██████████| 956/956 [00:11<00:00, 81.61it/s, loss=6.18e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.89it/s, loss=5.19e+4]\n",
      "epoch 47: 100%|██████████| 956/956 [00:12<00:00, 78.58it/s, loss=6.19e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.20it/s, loss=5.2e+4] \n",
      "epoch 48: 100%|██████████| 956/956 [00:11<00:00, 80.78it/s, loss=6.17e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.01it/s, loss=5.18e+4]\n",
      "epoch 49: 100%|██████████| 956/956 [00:11<00:00, 81.44it/s, loss=6.16e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 142.81it/s, loss=5.19e+4]\n",
      "epoch 50: 100%|██████████| 956/956 [00:11<00:00, 79.77it/s, loss=6.14e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.50it/s, loss=5.19e+4]\n",
      "epoch 51: 100%|██████████| 956/956 [00:11<00:00, 82.75it/s, loss=6.1e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.98it/s, loss=5.18e+4]\n",
      "epoch 52: 100%|██████████| 956/956 [00:12<00:00, 78.32it/s, loss=6.13e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.38it/s, loss=5.2e+4] \n",
      "epoch 53: 100%|██████████| 956/956 [00:11<00:00, 82.59it/s, loss=6.07e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 143.43it/s, loss=5.19e+4]\n",
      "epoch 54: 100%|██████████| 956/956 [00:11<00:00, 81.06it/s, loss=6.06e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.43it/s, loss=5.18e+4]\n",
      "epoch 55: 100%|██████████| 956/956 [00:11<00:00, 83.24it/s, loss=6.05e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.01it/s, loss=5.19e+4]\n",
      "epoch 56: 100%|██████████| 956/956 [00:11<00:00, 80.06it/s, loss=6.03e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.68it/s, loss=5.16e+4]\n",
      "epoch 57: 100%|██████████| 956/956 [00:11<00:00, 82.41it/s, loss=6.03e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.29it/s, loss=5.15e+4]\n",
      "epoch 58: 100%|██████████| 956/956 [00:11<00:00, 82.61it/s, loss=5.99e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 145.97it/s, loss=5.13e+4]\n",
      "epoch 59: 100%|██████████| 956/956 [00:11<00:00, 82.53it/s, loss=5.95e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.21it/s, loss=5.12e+4]\n",
      "epoch 60: 100%|██████████| 956/956 [00:12<00:00, 78.11it/s, loss=5.92e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.79it/s, loss=5.08e+4]\n",
      "epoch 61: 100%|██████████| 956/956 [00:11<00:00, 83.29it/s, loss=5.86e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.32it/s, loss=5.02e+4]\n",
      "epoch 62: 100%|██████████| 956/956 [00:11<00:00, 83.28it/s, loss=5.8e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 145.98it/s, loss=4.94e+4]\n",
      "epoch 63: 100%|██████████| 956/956 [00:11<00:00, 82.39it/s, loss=5.74e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 154.18it/s, loss=4.87e+4]\n",
      "epoch 64: 100%|██████████| 956/956 [00:12<00:00, 73.95it/s, loss=5.68e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.78it/s, loss=4.77e+4]\n",
      "epoch 65: 100%|██████████| 956/956 [00:11<00:00, 82.82it/s, loss=5.59e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 142.59it/s, loss=4.7e+4] \n",
      "epoch 66: 100%|██████████| 956/956 [00:11<00:00, 82.33it/s, loss=5.53e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.93it/s, loss=4.64e+4]\n",
      "epoch 67: 100%|██████████| 956/956 [00:11<00:00, 79.85it/s, loss=5.47e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.20it/s, loss=4.59e+4]\n",
      "epoch 68: 100%|██████████| 956/956 [00:12<00:00, 76.97it/s, loss=5.41e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 137.08it/s, loss=4.53e+4]\n",
      "epoch 69: 100%|██████████| 956/956 [00:12<00:00, 77.48it/s, loss=5.37e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 140.87it/s, loss=4.5e+4] \n",
      "epoch 70: 100%|██████████| 956/956 [00:11<00:00, 79.99it/s, loss=5.36e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 141.49it/s, loss=4.48e+4]\n",
      "epoch 71: 100%|██████████| 956/956 [00:11<00:00, 80.38it/s, loss=5.34e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.69it/s, loss=4.46e+4]\n",
      "epoch 72: 100%|██████████| 956/956 [00:11<00:00, 81.21it/s, loss=5.29e+4]\n",
      "valid: 100%|██████████| 410/410 [00:03<00:00, 130.37it/s, loss=4.43e+4]\n",
      "epoch 73: 100%|██████████| 956/956 [00:11<00:00, 79.72it/s, loss=5.28e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.13it/s, loss=4.43e+4]\n",
      "epoch 74: 100%|██████████| 956/956 [00:11<00:00, 81.98it/s, loss=5.28e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.85it/s, loss=4.42e+4]\n",
      "epoch 75: 100%|██████████| 956/956 [00:12<00:00, 78.54it/s, loss=5.24e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.93it/s, loss=4.39e+4]\n",
      "epoch 76: 100%|██████████| 956/956 [00:11<00:00, 81.02it/s, loss=5.24e+4]\n",
      "valid: 100%|██████████| 410/410 [00:03<00:00, 129.02it/s, loss=4.4e+4] \n",
      "epoch 77: 100%|██████████| 956/956 [00:11<00:00, 80.33it/s, loss=5.22e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.20it/s, loss=4.4e+4] \n",
      "epoch 78: 100%|██████████| 956/956 [00:12<00:00, 76.02it/s, loss=5.19e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 145.16it/s, loss=4.37e+4]\n",
      "epoch 79: 100%|██████████| 956/956 [00:11<00:00, 80.86it/s, loss=5.18e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 141.48it/s, loss=4.36e+4]\n",
      "epoch 80: 100%|██████████| 956/956 [00:11<00:00, 80.68it/s, loss=5.17e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 136.69it/s, loss=4.37e+4]\n",
      "epoch 81: 100%|██████████| 956/956 [00:12<00:00, 78.70it/s, loss=5.14e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.52it/s, loss=4.37e+4]\n",
      "epoch 82: 100%|██████████| 956/956 [00:11<00:00, 80.95it/s, loss=5.14e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.23it/s, loss=4.35e+4]\n",
      "epoch 83: 100%|██████████| 956/956 [00:11<00:00, 80.84it/s, loss=5.11e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 142.45it/s, loss=4.34e+4]\n",
      "epoch 84: 100%|██████████| 956/956 [00:11<00:00, 80.44it/s, loss=5.1e+4] \n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 144.74it/s, loss=4.33e+4]\n",
      "epoch 85: 100%|██████████| 956/956 [00:12<00:00, 78.79it/s, loss=5.08e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.07it/s, loss=4.3e+4] \n",
      "epoch 86: 100%|██████████| 956/956 [00:11<00:00, 81.94it/s, loss=5.08e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 146.14it/s, loss=4.3e+4] \n",
      "epoch 87: 100%|██████████| 956/956 [00:12<00:00, 77.79it/s, loss=5.08e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 152.84it/s, loss=4.3e+4] \n",
      "epoch 88: 100%|██████████| 956/956 [00:12<00:00, 78.34it/s, loss=5.08e+4]\n",
      "valid: 100%|██████████| 410/410 [00:03<00:00, 135.12it/s, loss=4.29e+4]\n",
      "epoch 89: 100%|██████████| 956/956 [00:11<00:00, 80.40it/s, loss=5.06e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 144.49it/s, loss=4.28e+4]\n",
      "epoch 90: 100%|██████████| 956/956 [00:11<00:00, 80.55it/s, loss=5.03e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 145.58it/s, loss=4.27e+4]\n",
      "epoch 91: 100%|██████████| 956/956 [00:12<00:00, 76.06it/s, loss=5.05e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 149.09it/s, loss=4.28e+4]\n",
      "epoch 92: 100%|██████████| 956/956 [00:12<00:00, 78.90it/s, loss=5.01e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 144.98it/s, loss=4.26e+4]\n",
      "epoch 93: 100%|██████████| 956/956 [00:12<00:00, 78.63it/s, loss=5.03e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 148.26it/s, loss=4.25e+4]\n",
      "epoch 94: 100%|██████████| 956/956 [00:11<00:00, 81.47it/s, loss=5.02e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 150.74it/s, loss=4.26e+4]\n",
      "epoch 95: 100%|██████████| 956/956 [00:11<00:00, 81.08it/s, loss=5.01e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 142.99it/s, loss=4.23e+4]\n",
      "epoch 96: 100%|██████████| 956/956 [00:11<00:00, 82.68it/s, loss=4.98e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 138.25it/s, loss=4.24e+4]\n",
      "epoch 97: 100%|██████████| 956/956 [00:12<00:00, 76.09it/s, loss=4.99e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.03it/s, loss=4.23e+4]\n",
      "epoch 98: 100%|██████████| 956/956 [00:11<00:00, 82.16it/s, loss=4.99e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 147.14it/s, loss=4.23e+4]\n",
      "epoch 99: 100%|██████████| 956/956 [00:11<00:00, 81.36it/s, loss=4.99e+4]\n",
      "valid: 100%|██████████| 410/410 [00:03<00:00, 128.14it/s, loss=4.23e+4]\n",
      "epoch 100: 100%|██████████| 956/956 [00:11<00:00, 81.82it/s, loss=4.96e+4]\n",
      "valid: 100%|██████████| 410/410 [00:02<00:00, 151.18it/s, loss=4.2e+4] \n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "# Utilize Pytorch-WideDeep Library\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# TabPreprocessor: Preprocess the data \n",
    "# Define the 'column set up'\n",
    "cat_embed_cols = [\n",
    "    \"month\",\n",
    "    \"town\",\n",
    "    \"flat_model_type\",\n",
    "    \"storey_range\",\n",
    "]\n",
    "continuous_cols = [\n",
    "    \"dist_to_nearest_stn\",\n",
    "    \"dist_to_dhoby\",\n",
    "    \"degree_centrality\",\n",
    "    \"eigenvector_centrality\",\n",
    "    \"remaining_lease_years\",\n",
    "    \"floor_area_sqm\",\n",
    "]\n",
    "\n",
    "# Prepare the data\n",
    "tab_preprocessor = TabPreprocessor(cat_embed_cols=cat_embed_cols, \n",
    "                                    continuous_cols=continuous_cols, \n",
    "                                    cols_to_scale=continuous_cols)\n",
    "X_tab_train = tab_preprocessor.fit_transform(train_df)\n",
    "X_tab_test = tab_preprocessor.transform(test_df)\n",
    "\n",
    "# TabMlp: Build the TabMlp model\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[200, 100]\n",
    ")\n",
    "\n",
    "# Create the WideDeep model with TabMlp as the deeptabular component\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "\n",
    "# Trainer: Train the model\n",
    "# Getting the target values\n",
    "target_train = train_df['resale_price'].values\n",
    "target_test = test_df['resale_price'].values\n",
    "\n",
    "# Train and validate\n",
    "trainer = Trainer(model=model, objective=\"rmse\", num_workers=0, seed=SEED)\n",
    "trainer.fit(\n",
    "    X_tab=X_tab_train,\n",
    "    target=target_train,\n",
    "    n_epochs=100,\n",
    "    batch_size=64, \n",
    "    val_split=0.3, # 70-30 split\n",
    ")\n",
    "\n",
    "# Train DF Shape\n",
    "# (87370, 14)\n",
    "# 70-30 Split means 61159 rows for training and 26211 rows for validation\n",
    "# Batch size of 64 further means 956 batches for training and 410 batches for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:02<00:00, 409.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Model Performance on Test Set:\n",
      "Test RMSE: 118457.3092 (4 d.p.)\n",
      "Test R²  : 0.5097      (4 d.p.)\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# make predictions on the test set\n",
    "preds = trainer.predict(X_tab=X_tab_test).ravel()\n",
    "\n",
    "# Test DF Shape\n",
    "# (72183, 14)\n",
    "# 1128 Iterations because 72183 rows / 64 batch size = 1128\n",
    "\n",
    "# Calculate the Root Mean Squared Error (RMSE) between the predicted and actual target values\n",
    "rmse = np.sqrt(mean_squared_error(target_test, preds))\n",
    "\n",
    "# Calculate the R-squared (R²) score, which measures the goodness of fit of the model\n",
    "r2 = r2_score(target_test, preds)\n",
    "\n",
    "# Print the RMSE and R² scores to evaluate the model's performance on the test dataset\n",
    "print()\n",
    "print('-------------------------------')\n",
    "print('Model Performance on Test Set:')\n",
    "print(f\"Test RMSE: {round(rmse,4)} (4 d.p.)\")\n",
    "print(f\"Test R²  : {round(r2,4)}      (4 d.p.)\")\n",
    "print('-------------------------------')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
