{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp6KAX1UXqaK"
   },
   "source": [
    "CS4001/4042 Assignment 1, Part B, Q4\n",
    "---\n",
    "\n",
    "Model degradation is a common issue faced when deploying machine learning models (including neural networks) in the real world. New data points could exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. For instance, housing prices in Singapore have been increasing and the Singapore government has introduced 3 rounds of cooling measures over the past years (16 December 2021, 30 September 2022, 27 April 2023).\n",
    "\n",
    "In such situations, the distribution of the new data points could differ from the original data distribution which the models were trained on. Recall that machine learning models often work with the assumption that the test distribution should be similar to train distribution. When this assumption is violated, model performance will be adversely impacted.  In the last part of this assignment, we will investigate to what extent model degradation has occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsfKoCAMj9uo"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rjdf67uIarDX"
   },
   "source": [
    "Your co-investigators used a linear regression model to rapidly test out several combinations of train/test splits and shared with you their findings in a brief report attached in Appendix A below. You wish to investigate whether your deep learning model corroborates with their findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "M3-BW2LW4Icq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alibi-detect in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.4)\n",
      "Requirement already satisfied: dill<0.4.0,>=0.3.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (0.3.7)\n",
      "Requirement already satisfied: numba!=0.54.0,<0.58.0,>=0.50.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (0.57.1)\n",
      "Requirement already satisfied: catalogue<3.0.0,>=2.0.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (2.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wei kang\\appdata\\roaming\\python\\python39\\site-packages (from alibi-detect) (4.8.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (1.24.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.0.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (4.33.2)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.8.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (1.10.12)\n",
      "Requirement already satisfied: toml<1.0.0,>=0.10.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (0.10.2)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (3.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (4.66.1)\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=3.2.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (4.8.0.76)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=0.20.2 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (1.3.1)\n",
      "Requirement already satisfied: scikit-image!=0.17.1,<0.22,>=0.14.2 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (0.21.0)\n",
      "Requirement already satisfied: Pillow<10.0.0,>=5.4.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (9.5.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.0.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from alibi-detect) (2.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.4.5)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (6.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wei kang\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (3.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\wei kang\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from numba!=0.54.0,<0.58.0,>=0.50.0->alibi-detect) (0.40.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas<3.0.0,>=1.0.0->alibi-detect) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas<3.0.0,>=1.0.0->alibi-detect) (2023.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (3.2.0)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image!=0.17.1,<0.22,>=0.14.2->alibi-detect) (0.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image!=0.17.1,<0.22,>=0.14.2->alibi-detect) (2023.9.18)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image!=0.17.1,<0.22,>=0.14.2->alibi-detect) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image!=0.17.1,<0.22,>=0.14.2->alibi-detect) (2.31.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-image!=0.17.1,<0.22,>=0.14.2->alibi-detect) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<2.0.0,>=0.20.2->alibi-detect) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<2.0.0,>=0.20.2->alibi-detect) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\wei kang\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.28.1->alibi-detect) (0.4.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (2023.8.8)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (0.17.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (3.12.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (0.3.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (6.0.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wei kang\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers<5.0.0,>=4.0.0->alibi-detect) (2023.9.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\wei kang\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib<4.0.0,>=3.0.0->alibi-detect) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wei kang\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Wei Kang\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install alibi-detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E7dD3Ihi4GF9"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "from alibi_detect.cd import TabularDrift\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJjXNMOqcHVJ"
   },
   "source": [
    "> Evaluate your model from B1 on data from year 2022 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fOUcXL5OXASY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:07:45,420 - {pytorch_tabular.tabular_model:105} - INFO - Experiment Tracking is turned off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique years in train_df_2022: 2017, 2018, 2019\n",
      "Unique years in validation_df_2022: 2020\n",
      "Unique years in test_df_2022: 2022\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "2023-10-09 13:07:45,464 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders\n",
      "2023-10-09 13:07:45,471 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for regression task\n",
      "2023-10-09 13:07:45,588 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: CategoryEmbeddingModel\n",
      "2023-10-09 13:07:45,659 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-10-09 13:07:45,714 - {pytorch_tabular.tabular_model:573} - INFO - Auto LR Find Started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8061424028452982b7ed2d89c97398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.5754399373371567\n",
      "Restoring states from the checkpoint path at c:\\Users\\Wei Kang\\Desktop\\Individual Assignment\\.lr_find_478720eb-7fdd-4e3f-be98-452cdbfd85a2.ckpt\n",
      "Restored all states from the checkpoint file at c:\\Users\\Wei Kang\\Desktop\\Individual Assignment\\.lr_find_478720eb-7fdd-4e3f-be98-452cdbfd85a2.ckpt\n",
      "2023-10-09 13:07:52,881 - {pytorch_tabular.tabular_model:575} - INFO - Suggested LR: 0.5754399373371567. For plot and detailed analysis, use `find_learning_rate` method.\n",
      "2023-10-09 13:07:52,883 - {pytorch_tabular.tabular_model:582} - INFO - Training Started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ CategoryEmbeddingBackbone │  2.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer          │  1.5 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head             │ LinearHead                │     51 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ MSELoss                   │      0 │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ CategoryEmbeddingBackbone │  2.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer          │  1.5 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head             │ LinearHead                │     51 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ MSELoss                   │      0 │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.5 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.5 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.5 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.5 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff611df3897c42b081a0b2d556880cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:08:35,197 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed\n",
      "2023-10-09 13:08:35,198 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c455d9078b0b446c9d6109931842699b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       16267038720.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       16267038720.0       </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      16267038720.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      16267038720.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d5780cdde7418b969895e5b5a7edb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 2022 Test Set: 127542.3017\n",
      "R² for 2022 Test Set: 0.4388\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Filtering data for evaluation on 2022 data\n",
    "train_df_2019 = df[df['year'] <= 2019]\n",
    "validation_df_2020 = df[df['year'] == 2020]\n",
    "test_df_2022 = df[df['year'] == 2022]\n",
    "\n",
    "# Sanity Check: Get unique values \n",
    "train_unique_years_2022 = ', '.join(map(str, train_df_2019['year'].unique()))\n",
    "validation_unique_years_2022 = ', '.join(map(str, validation_df_2020['year'].unique()))\n",
    "test_unique_years_2022 = ', '.join(map(str, test_df_2022['year'].unique()))\n",
    "\n",
    "# Sanity Check: Print the formatted unique values\n",
    "print(f\"Unique years in train_df_2022: {train_unique_years_2022}\")\n",
    "print(f\"Unique years in validation_df_2022: {validation_unique_years_2022}\")\n",
    "print(f\"Unique years in test_df_2022: {test_unique_years_2022}\\n\\n\")\n",
    "\n",
    "# Define the DataConfig \n",
    "data_config= DataConfig(\n",
    "    target=['resale_price'], \n",
    "    continuous_cols=['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm'],\n",
    "    categorical_cols=['month', 'town', 'flat_model_type', 'storey_range']\n",
    ")\n",
    "\n",
    "# Define the TrainerConfig \n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True, \n",
    "    batch_size=1024, \n",
    "    max_epochs=50,  \n",
    ")\n",
    "\n",
    "# Define the CategoryEmbeddingModelConfig \n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"regression\",\n",
    "    layers=\"50\" \n",
    ")\n",
    "\n",
    "# Create the OptimizerConfig \n",
    "optimizer_config = OptimizerConfig(optimizer=\"Adam\")\n",
    "\n",
    "# Create the TabularModel\n",
    "model_2022 = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")\n",
    "\n",
    "# Train the model using 2022 dataset\n",
    "model_2022.fit(train=train_df_2019, validation=validation_df_2020, seed=SEED)\n",
    "\n",
    "# Evaluate the model on the 2022 test dataset and store the results\n",
    "result_2022 = model_2022.evaluate(test_df_2022)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(result_2022)\n",
    "\n",
    "# Calculate the Root Mean Square Error (RMSE) from the test results\n",
    "rmse_2022 = math.sqrt(result_2022[0]['test_mean_squared_error'])\n",
    "\n",
    "# Get predictions for the 2022 test dataset\n",
    "pred_2022_df = model_2022.predict(test_df_2022)\n",
    "actual_values_2022 = test_df_2022['resale_price'].values  \n",
    "predicted_values_2022 = pred_2022_df[\"resale_price_prediction\"].values  \n",
    "\n",
    "# Calculate the R² for 2022 Test Set\n",
    "r2_2022 = r2_score(actual_values_2022, predicted_values_2022)\n",
    "\n",
    "# Print values\n",
    "print(f\"RMSE for 2022 Test Set: {round(rmse_2022,4)}\")\n",
    "print(f\"R² for 2022 Test Set: {round(r2_2022,4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gsbs0iMiaUy-"
   },
   "source": [
    "> Evaluate your model from B1 on data from year 2023 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B4FLRQfBaRS-"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66057184deab475a8ab4a32816e2ed43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       24701364224.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       24701364224.0       </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      24701364224.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      24701364224.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3935ceb24b1b4b75b14219551142627f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 2023 Test Set: 157166.6766\n",
      "R² for 2023 Test Set: 0.1621\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "\n",
    "# 1. Filter data for year 2023\n",
    "test_2023_df = df[df['year'] == 2023]\n",
    "\n",
    "# 2. Evaluate the model on the 2023 dataset and store the results\n",
    "result_2023 = model_2022.evaluate(test_2023_df)\n",
    "\n",
    "# 3. Get predictions for the 2023 dataset\n",
    "pred_2023_df = model_2022.predict(test_2023_df)\n",
    "\n",
    "# Extract the actual and predicted values for 2023\n",
    "actual_values_2023 = test_2023_df['resale_price'].values\n",
    "predicted_values_2023 = pred_2023_df[\"resale_price_prediction\"].values\n",
    "\n",
    "# Calculate the Root Mean Square Error (RMSE) from the test results\n",
    "rmse_2023 = math.sqrt(result_2023[0]['test_mean_squared_error'])\n",
    "print(f\"RMSE for 2023 Test Set: {round(rmse_2023,4)}\")\n",
    "\n",
    "# Calculate the R² for 2023 data\n",
    "r2_2023 = r2_score(actual_values_2023, predicted_values_2023)\n",
    "\n",
    "# Print the R² value for the 2023 data  \n",
    "print(f\"R² for 2023 Test Set: {round(r2_2023,4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11mU8sxcaSAP"
   },
   "source": [
    "> Did model degradation occur for the deep learning model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nviGacm6aSlf"
   },
   "source": [
    "### Yes. \n",
    "\n",
    "Model degradation, within the machine learning domain, is the deterioration of a model's performance when exposed to new data that may not adhere to the original training data's distribution. This drop in efficacy is especially pronounced when the incoming data significantly deviates from patterns present during training.\n",
    "\n",
    "Given the results:\n",
    "\n",
    "- R² for 2022: 0.4388\n",
    "- R² for 2023: 0.1621\n",
    "\n",
    "There's a clear decrease in the R² value from 2022 to 2023, indicating a decline in the model's performance. In the context of Singapore's housing prices and the government's cooling measures between December 2021 and April 2023, it's plausible that the data distribution in 2023 differs from that of the original training set (data until 2019). This suggests that \"concept drift\" might have occurred, where the relationship between features and the target variable (resale price) has changed over time. (More to be discussed below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyiP8gBAcABD"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruldtSTDcYzt"
   },
   "source": [
    "Model degradation could be caused by [various data distribution shifts](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html#data-shift-types): covariate shift (features), label shift and/or concept drift (altered relationship between features and labels).\n",
    "There are various conflicting terminologies in the [literature](https://www.sciencedirect.com/science/article/pii/S0950705122002854#tbl1). Let’s stick to this reference for this assignment.\n",
    "\n",
    "> Using the **Alibi Detect** library, apply the **TabularDrift** function with the training data (year 2019 and before) used as the reference and **detect which features have drifted** in the 2023 test dataset. Before running the statistical tests, ensure you **sample 1000 data points** each from the train and test data. Do not use the whole train/test data. (Hint: use this example as a guide https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['month', 'year', 'town', 'full_address', 'nearest_stn',\n",
      "       'dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality',\n",
      "       'eigenvector_centrality', 'flat_model_type', 'remaining_lease_years',\n",
      "       'floor_area_sqm', 'storey_range', 'resale_price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Was there a drift on Test Set? Yes!\n",
      "==================================================\n",
      "==================================================\n",
      "Printing the p-values for each feature:\n",
      "\n",
      "month -- Drift? Yes! -- Chi2 430.3365 -- p-value 0.0000\n",
      "year -- Drift? Yes! -- Chi2 2000.0000 -- p-value 0.0000\n",
      "town -- Drift? No! -- Chi2 33.1777 -- p-value 0.1267\n",
      "full_address -- Drift? No! -- Chi2 1799.3334 -- p-value 0.1532\n",
      "nearest_stn -- Drift? Yes! -- Chi2 110.0444 -- p-value 0.0080\n",
      "dist_to_nearest_stn -- Drift? No! -- K-S 0.0350 -- p-value 0.5606\n",
      "dist_to_dhoby -- Drift? No! -- K-S 0.0590 -- p-value 0.0591\n",
      "degree_centrality -- Drift? No! -- K-S 0.0380 -- p-value 0.4547\n",
      "eigenvector_centrality -- Drift? No! -- K-S 0.0560 -- p-value 0.0837\n",
      "flat_model_type -- Drift? Yes! -- Chi2 62.1219 -- p-value 0.0011\n",
      "remaining_lease_years -- Drift? Yes! -- K-S 0.1630 -- p-value 0.0000\n",
      "floor_area_sqm -- Drift? Yes! -- K-S 0.0620 -- p-value 0.0410\n",
      "storey_range -- Drift? Yes! -- Chi2 27.8423 -- p-value 0.0095\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def print_delimiter(character=\"=\", length=50):\n",
    "    print(character * length)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"hdb_price_prediction.csv\")\n",
    "\n",
    "# Split data into training set (year 2019 and before) and 2023 test set\n",
    "X_train = df[df['year'] <= 2019].drop(columns=['resale_price'])\n",
    "X_test_2023 = df[df['year'] == 2023].drop(columns=['resale_price'])\n",
    "\n",
    "# Sample 1000 data points from the training data (year 2019 and before) and 2023 test data\n",
    "X_train_sample = X_train.sample(1000, random_state=SEED)\n",
    "X_test_2023_sample = X_test_2023.sample(1000, random_state=SEED)\n",
    "\n",
    "feature_names = [\"month\",\"year\",\"town\",\"full_address\",\"nearest_stn\",\"dist_to_nearest_stn\",\"dist_to_dhoby\",\"degree_centrality\",\n",
    "                 \"eigenvector_centrality\",\"flat_model_type\",\"remaining_lease_years\",\"floor_area_sqm\",\"storey_range\"]\n",
    "\n",
    "labels = ['No!', 'Yes!']\n",
    "\n",
    "category_map = {\n",
    "    0: None, # month \n",
    "    1: None, # year\n",
    "    2: None, # town\n",
    "    3: None, # full_address\n",
    "    4: None, # nearest_stn\n",
    "    9: None, # flat_model_type\n",
    "    12: None # storey_range\n",
    "}\n",
    "\n",
    "categories_per_feature = {f: None for f in list(category_map.keys())}\n",
    "cd = TabularDrift(X_train_sample.values, p_val=.05, categories_per_feature=categories_per_feature)\n",
    "preds = cd.predict(X_test_2023_sample.values)\n",
    "fpreds = cd.predict(X_test_2023_sample.values, drift_type='feature')\n",
    "print_delimiter()\n",
    "# Check for Drift on the Test Set:\n",
    "print('Was there a drift on Test Set? {}'.format(labels[preds['data']['is_drift']]))\n",
    "print_delimiter()\n",
    "print_delimiter()\n",
    "\n",
    "print('Printing the p-values for each feature:', end = '\\n\\n')\n",
    "# Inspect Results for Each Feature:\n",
    "for f in range(cd.n_features):\n",
    "    # Check if the feature is categorical or continuous\n",
    "    stat = 'Chi2' if f in list(categories_per_feature.keys()) else 'K-S'\n",
    "    fname = feature_names[f]\n",
    "    is_drift = fpreds['data']['is_drift'][f]\n",
    "    stat_val, p_val = fpreds['data']['distance'][f], fpreds['data']['p_val'][f]\n",
    "    print(f'{fname} -- Drift? {labels[is_drift]} -- {stat} {stat_val:.4f} -- p-value {p_val:.4f}')\n",
    "\n",
    "print_delimiter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xmj3qq3PkJUf"
   },
   "source": [
    "> Assuming that the flurry of housing measures have made an impact on the relationship between all the features and resale_price (i.e. P(Y|X) changes), which type of data distribution shift possibly led to model degradation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UOsX4JqkZ9S"
   },
   "source": [
    "# **Concept Shift**\n",
    "\n",
    "Given the recent developments in the Singapore housing market, notably the introduction of several rounds of cooling measures, it's imperative to understand the type of data distribution shift that might have occurred, resulting in model degradation.\n",
    "\n",
    "### **1. Covariate Shift**\n",
    "- **Definition**: The distribution of input features changes, but the relationship between the input and output remains the same.\n",
    "- **Context**: This shift would imply that the distribution of features like `town`, `flat_model_type`, `storey_range`, etc. has changed over time. However, despite these changes, the way they relate to the `resale_price` remains consistent. Specifically, if only the type of flats being sold or the towns they are in have changed distribution, but their impact on the `resale_price` remained the same, it would be indicative of a covariate shift.\n",
    "\n",
    "### **2. Label Shift**\n",
    "- **Definition**: The distribution of the labels (or target) changes, but the relationship between features and labels stays the same.\n",
    "- **Context**: This would mean that the distribution of the `resale_price` itself has changed, but the way the features predict this price remains the same. If the resale prices in the market generally went up or down, but the influence of features on this price stayed consistent, it would indicate a label shift.\n",
    "\n",
    "### **3. Concept Drift**\n",
    "- **Definition**: The relationship between input features and the target variable changes.\n",
    "- **Context**: This is when the underlying relationship between the features and the `resale_price` changes. Given the cooling measures' assumed impact on all the features and resale prices in the Singapore housing market, this would signify a concept drift.\n",
    "\n",
    "### **Conclusion**\n",
    "Given that the housing measures have influenced the relationship between the features and the `resale_price` and that the functional relationship between the features and the target variable, i.e., \\(P(Y|X)\\), has changed, this suggests that **Concept Drift** is the most probable shift in this scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM2OoOJdkZj1"
   },
   "source": [
    "> From your analysis via TabularDrift, which features contribute to this shift?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3licBjskdLL"
   },
   "source": [
    "# **Drifted Features**\n",
    "\n",
    "The results from the `TabularDrift` analysis are as follows:\n",
    "\n",
    "| Feature                  | Drift Detected | Test Used | Test Statistic | P-Value   |\n",
    "|--------------------------|----------------|-----------|----------------|-----------|\n",
    "| month              | Yes            | Chi2      | 430.3365       | 0.0000 |\n",
    "| year               | Yes            | Chi2      | 2000.0000      | 0.0000 |\n",
    "| town                     | No             | Chi2      | 33.1777        | 0.1267    |\n",
    "| full_address             | No             | Chi2      | 1799.3334      | 0.1532    |\n",
    "| ***nearest_stn***              | Yes            | Chi2      | 110.0444       | ***0.0080*** |\n",
    "| dist_to_nearest_stn      | No             | K-S       | 0.0350         | 0.5606    |\n",
    "| dist_to_dhoby            | No             | K-S       | 0.0590         | 0.0591    |\n",
    "| degree_centrality        | No             | K-S       | 0.0380         | 0.4547    |\n",
    "| eigenvector_centrality   | No             | K-S       | 0.0560         | 0.0837    |\n",
    "| ***flat_model_type***    | Yes            | Chi2      | 62.1219        | ***0.0011*** |\n",
    "| ***remaining_lease_years*** | Yes        | K-S       | 0.1630         | ***0.0000*** |\n",
    "| ***floor_area_sqm***     | Yes            | K-S       | 0.0620         | ***0.0410*** |\n",
    "| ***storey_range***       | Yes            | Chi2      | 27.8423        | ***0.0095*** |\n",
    "\n",
    "Based on our results, the features that show significant drift (p-value < 0.05) include:\n",
    "\n",
    "- month\n",
    "- year\n",
    "- nearest_stn\n",
    "- flat_model_type\n",
    "- remaining_lease_years\n",
    "- floor_area_sqm\n",
    "- storey_range\n",
    "\n",
    "Among these, month and year are time-based and expected to change, so the primary contributors from a model perspective might be the rest of the features, i.e. (`nearest_stn`, `flat_model_type`, `remaining_lease_years`, `floor_area_sqm`, `storey_range`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB3fSvKskhEJ"
   },
   "source": [
    "> Suggest 1 way to address model degradation and implement it, showing improved test R2 for year 2023.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UpP7K6Tkglx"
   },
   "source": [
    "# Addressing Model Degradation\n",
    "\n",
    "The housing market has seen significant changes due to government interventions and cooling measures. These external forces have resulted in the observed model degradation. To combat this and maintain a robust predictive model, we propose the following strategies:\n",
    "\n",
    "### **1. Adaptive Training with Weighted Samples**\n",
    "\n",
    "#### **Insights from the Appendix**:\n",
    "- Models trained closer to the test year tend to perform better, indicating the importance of using recent data for training.\n",
    "- While older data provides a historical context, it seems to lose predictive power for future years, especially in the face of external changes like government interventions.\n",
    "\n",
    "#### **Rationale:**\n",
    "- **Recent Market Changes:** Recent data represents the current state of the market, affected by new policies and economic conditions. Therefore, it's imperative to adapt the model by incorporating the most recent data.  Models trained on recent data perform better. This is evident from the appendix where models trained closer to 2021 had higher R2 values.\n",
    "- **Balancing Historical and Current Data:** While it's essential to capture long-term trends, recent data provides crucial information about the market's current state. Weighing samples based on their recency allows the model to benefit from both historical context and current dynamics.\n",
    "\n",
    "#### **Strategy:**\n",
    "- **Data Inclusion:** Incorporate data post-2019 till 2022 into the training set. We use 90% of the data up to 2022 for training, and 10% for validation since validation is not very important here and we are mainly concerned with addressing the model degradation issue by ensuring that the model can generalize well to new data (years 2023). \n",
    "- **Weight Assignment:** Weigh the samples in a manner where newer samples are assigned higher importance during training. We do this through a weighted random sampler. This ensures that the model doesn't overlook historical data but emphasizes recent market changes more. (e.g., **0.000004 for 2017** and **0.000015 for 2022** after normalization of weights.)\n",
    "\n",
    "#### **Result:**\n",
    "- **Increase in R2 Value:** Our strategy resulted in an improvement in R2 Value from **`0.1621`** to **`0.6038`**, demonstrating the effectiveness of training with the most recent data, giving precedence to recent data and balancing it with historical insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EqFinZObcYXu"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year    Weight\n",
       "0  2017  0.000004\n",
       "1  2018  0.000005\n",
       "2  2019  0.000006\n",
       "3  2020  0.000008\n",
       "4  2021  0.000010\n",
       "5  2022  0.000015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:33:53,640 - {pytorch_tabular.tabular_model:105} - INFO - Experiment Tracking is turned off\n",
      "Global seed set to 42\n",
      "2023-10-09 13:33:53,671 - {pytorch_tabular.tabular_model:473} - INFO - Preparing the DataLoaders\n",
      "2023-10-09 13:33:53,681 - {pytorch_tabular.tabular_datamodule:290} - INFO - Setting up the datamodule for regression task\n",
      "2023-10-09 13:33:53,842 - {pytorch_tabular.tabular_model:521} - INFO - Preparing the Model: CategoryEmbeddingModel\n",
      "2023-10-09 13:33:53,875 - {pytorch_tabular.tabular_model:268} - INFO - Preparing the Trainer\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2023-10-09 13:33:53,932 - {pytorch_tabular.tabular_model:573} - INFO - Auto LR Find Started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a24ad3e5dc04a2badeaa5e2b6e4e5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.5754399373371567\n",
      "Restoring states from the checkpoint path at c:\\Users\\Wei Kang\\Desktop\\Individual Assignment\\.lr_find_eea41a8e-a11f-420e-b2cd-c9828ba7397e.ckpt\n",
      "Restored all states from the checkpoint file at c:\\Users\\Wei Kang\\Desktop\\Individual Assignment\\.lr_find_eea41a8e-a11f-420e-b2cd-c9828ba7397e.ckpt\n",
      "2023-10-09 13:33:59,114 - {pytorch_tabular.tabular_model:575} - INFO - Suggested LR: 0.5754399373371567. For plot and detailed analysis, use `find_learning_rate` method.\n",
      "2023-10-09 13:33:59,116 - {pytorch_tabular.tabular_model:582} - INFO - Training Started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer          │  1.6 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head             │ LinearHead                │     51 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ MSELoss                   │      0 │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer          │  1.6 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head             │ LinearHead                │     51 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ MSELoss                   │      0 │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.6 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.6 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.6 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.6 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837924753f204d93a9f42a37ed026977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:34:57,169 - {pytorch_tabular.tabular_model:584} - INFO - Training the model completed\n",
      "2023-10-09 13:34:57,171 - {pytorch_tabular.tabular_model:1258} - INFO - Loading the best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eeddf200c5f46a5b83bbe31d54206d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       11679669248.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       11679669248.0       </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      11679669248.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      11679669248.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e263ddb55f9c4b998c6fe2cea0dfc332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'test_loss': 11679669248.0, 'test_mean_squared_error': 11679669248.0}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metrics</th>\n",
       "      <th>Old Values</th>\n",
       "      <th>New Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>157166.6766</td>\n",
       "      <td>108072.5185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R^2</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.6038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metrics   Old Values   New Values\n",
       "0    RMSE  157166.6766  108072.5185\n",
       "1     R^2       0.1621       0.6038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "# Use data up to 2022 for training, use 90% of this for training and 10% for validation \n",
    "data_up_to_2022 = df[df['year'] <= 2022]\n",
    "split_idx = int(0.9 * len(data_up_to_2022))  # Find the 90% mark\n",
    "\n",
    "train_up_to_2022 = data_up_to_2022.iloc[:split_idx]  # First 90%\n",
    "valid_up_to_2022 = data_up_to_2022.iloc[split_idx:]  # Last 10%\n",
    "\n",
    "test_2023_df = df[df['year'] == 2023]\n",
    "\n",
    "# Compute the sample weights using the year column of train_up_to_2022\n",
    "\n",
    "years = train_up_to_2022['year'].values\n",
    "weights = 1 / (2024 - years)  # Give more weight to recent years\n",
    "weights = weights / np.sum(weights)  # Normalize the weights to make them sum up to 1\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)  # Create the weighted sampler\n",
    "\n",
    "# Create a dictionary to store one weight per unique year\n",
    "unique_year_weights = {}\n",
    "\n",
    "# Loop through the unique years\n",
    "for year in np.unique(years):\n",
    "    # Find the index of the first occurrence of the current year\n",
    "    year_idx = np.where(years == year)[0][0]\n",
    "    # Get the weight of the first occurrence of the current year\n",
    "    unique_year_weights[year] = weights[year_idx]\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easier viewing\n",
    "unique_year_weights_df = pd.DataFrame(list(unique_year_weights.items()), columns=['Year', 'Weight'])\n",
    "\n",
    "# Display the DataFrame to see the weight assigned to each unique year\n",
    "display(unique_year_weights_df)\n",
    "\n",
    "# Define the DataConfig\n",
    "data_config_updated = DataConfig(\n",
    "    target=['resale_price'],\n",
    "    continuous_cols=['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm'],\n",
    "    categorical_cols=['month', 'town', 'flat_model_type', 'storey_range'],\n",
    "    validation_split=None\n",
    ")\n",
    "\n",
    "# Define the TrainerConfig\n",
    "trainer_config_updated = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    batch_size=1024,\n",
    "    max_epochs=50,\n",
    ")\n",
    "\n",
    "# Define the CategoryEmbeddingModelConfig\n",
    "model_config_updated = CategoryEmbeddingModelConfig(\n",
    "    task=\"regression\",\n",
    "    layers=\"50\"\n",
    ")\n",
    "\n",
    "# Create the OptimizerConfig\n",
    "optimizer_config_updated = OptimizerConfig(optimizer=\"Adam\")\n",
    "\n",
    "# Create the TabularModel with updated configurations\n",
    "model_updated = TabularModel(\n",
    "    data_config=data_config_updated,\n",
    "    model_config=model_config_updated,\n",
    "    optimizer_config=optimizer_config_updated,\n",
    "    trainer_config=trainer_config_updated,\n",
    ")\n",
    "\n",
    "# Train the updated model using data up to 2022 with the weighted sampler\n",
    "model_updated.fit(train=train_up_to_2022, validation=valid_up_to_2022, train_sampler=sampler, seed=SEED)\n",
    "\n",
    "# Evaluate the model on the 2023 test dataset and store the results\n",
    "result_updated = model_updated.evaluate(test_2023_df)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(result_updated)\n",
    "\n",
    "# Calculate the Root Mean Square Error (RMSE) from the test results\n",
    "rmse_updated = math.sqrt(result_updated[0]['test_mean_squared_error'])\n",
    "\n",
    "# Get predictions for the test dataset\n",
    "pred_2023_df = model_updated.predict(test_2023_df)\n",
    "actual_values_updated = test_2023_df['resale_price'].values\n",
    "predicted_values_updated = pred_2023_df[\"resale_price_prediction\"].values\n",
    "\n",
    "# Calculate the R² for the updated model\n",
    "r2_updated = r2_score(actual_values_updated, predicted_values_updated)\n",
    "\n",
    "# Old values\n",
    "rmse_2023 = math.sqrt(result_2023[0]['test_mean_squared_error'])\n",
    "r2_2023 = r2_score(actual_values_2023, predicted_values_2023)\n",
    "\n",
    "# New values\n",
    "rmse_updated = math.sqrt(result_updated[0]['test_mean_squared_error'])\n",
    "r2_updated = r2_score(actual_values_updated, predicted_values_updated)\n",
    "\n",
    "# Creating dataframe\n",
    "data = {\n",
    "    'Metrics': ['RMSE', 'R^2'],\n",
    "    'Old Values': [rmse_2023, r2_2023],\n",
    "    'New Values': [rmse_updated, r2_updated]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(data)\n",
    "comparison_df_rounded = comparison_df.round(4)\n",
    "display(comparison_df_rounded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQG9hZvAaq5g"
   },
   "source": [
    "### Appendix A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_ARi2OxbDuZ"
   },
   "source": [
    "Here are our results from a linear regression model. We used StandardScaler for continuous variables and OneHotEncoder for categorical variables.\n",
    "\n",
    "While 2021 data can be predicted well, test R2 dropped rapidly for 2022 and 2023.\n",
    "\n",
    "| Training set | Test set | Test R2 |\n",
    "|--------------|----------|---------|\n",
    "| Year <= 2020 | 2021     | 0.76    |\n",
    "| Year <= 2020 | **2022**     | 0.41    |\n",
    "| Year <= 2020 | **2023**     | **0.10**   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmv91FqgbI8h"
   },
   "source": [
    "Similarly, a model trained on 2017 data can predict 2018-2021 well (with slight degradation in performance for 2021), but drops drastically in 2022 and 2023.\n",
    "\n",
    "| Training set | Test set | Test R2 |\n",
    "|--------------|----------|---------|\n",
    "| 2017         | 2018     | 0.90    |\n",
    "|              | 2019     | 0.89    |\n",
    "|              | 2020     | 0.87    |\n",
    "|              | 2021     | 0.72    |\n",
    "|              | **2022**     | **0.37**    |\n",
    "|              | **2023**     | **0.09**    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayKGs106bI6S"
   },
   "source": [
    "With the test set fixed at year 2021, training on data from 2017-2020 still works well on the test data, with minimal degradation. Training sets closer to year 2021 generally do better.\n",
    "\n",
    "| Training set | Test set | Test R2 |\n",
    "|--------------|----------|---------|\n",
    "| 2020         | 2021     | 0.81    |\n",
    "| 2019         | 2021     | 0.75    |\n",
    "| 2018         | 2021     | 0.73    |\n",
    "| 2017         | 2021     | 0.72    |"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNWUKTcEICJ0j/YcXkGkr53",
   "collapsed_sections": [
    "wQG9hZvAaq5g"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
